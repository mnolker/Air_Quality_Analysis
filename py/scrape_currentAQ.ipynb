{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import time as tm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Excel File\n",
    "def city_data():\n",
    "    # Reading in File\n",
    "    filePath = \"../Data/ca_sites.csv\"\n",
    "    f = pd.read_csv(filePath)\n",
    "    # Creating DataFrame\n",
    "    dfCASites = pd.DataFrame(f)\n",
    "    \n",
    "    # Rename Column\n",
    "    dfCASites = dfCASites.rename(columns = {'City Name': 'City'}) \n",
    "    # Drop Duplicates\n",
    "    dfCASites = dfCASites.drop_duplicates(subset=['City'])\n",
    "\n",
    "    city = dfCASites['City'].tolist()\n",
    "    \n",
    "    return city "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    # Setting Up Splinter\n",
    "    executable_path = {'executable_path': \"C:/Windows/chromedriver\"}\n",
    "    return Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_info():\n",
    "    # Calling init_browser Function\n",
    "    browser = init_browser()\n",
    "    \n",
    "    # Calling city_list Function\n",
    "    city_list = city_data()\n",
    "    \n",
    "    # Creating List of Current AQI Data\n",
    "    currentAQI = []\n",
    "    \n",
    "    baseUrl = 'https://www.airnow.gov/?'\n",
    "    state = 'CA'\n",
    "    country = 'USA'\n",
    "    \n",
    "    for city in city_list:\n",
    "        # Building URL Query\n",
    "        url_air_now = baseUrl + 'city=' + city + '&state=' + state + '&country=' + country\n",
    "\n",
    "        # Visiting URL \n",
    "        browser.visit(url_air_now)\n",
    "        # Visiting the URL Takes Some Time, Using the Time Module to Slow Down the Run\n",
    "        tm.sleep(1)\n",
    "\n",
    "        # Scrape page into Soup\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Scraping Date & Time\n",
    "            aqUpdateTime = soup.find('span', class_='aq-updated-time')\n",
    "            currentDateTime = aqUpdateTime.text\n",
    "            currentTime = currentDateTime.rsplit('PST')[0] + 'PST'\n",
    "            currentDate = currentDateTime.rsplit('PST')[1]\n",
    "\n",
    "            # Scraping Current Pollutant\n",
    "            aqiItem = soup.find('div', class_='aqi')\n",
    "            aqi = aqiItem.find('b').text\n",
    "            pollutantItem = soup.find('div', class_='pollutant')\n",
    "            pollutant = pollutantItem.find('b').text\n",
    "\n",
    "            # Appending Dictionary to List\n",
    "            currentAQI.append({\"City\": city, \"Time\": currentTime, \"Date\": currentDate,\n",
    "                              \"Current AQI Value\": aqi, \"Current Pollutant\": pollutant})\n",
    "        except IndexError:\n",
    "            next\n",
    "            \n",
    "    # Closing Browser\n",
    "    browser.quit()\n",
    "  \n",
    "    # Creating DataFrame of currentAQI\n",
    "    dfAQI = pd.DataFrame(currentAQI)\n",
    "    # Removing White Space at the End of City Column\n",
    "    dfAQI['City'] = dfAQI['City'].str.strip()\n",
    "    \n",
    "    # Reading in ca-sites File\n",
    "    filePath = \"../Data/ca_sites.csv\"\n",
    "    f = pd.read_csv(filePath)\n",
    "    # Creating DataFrame\n",
    "    dfCASites = pd.DataFrame(f)\n",
    "    # Renaming Column to Help with Merge\n",
    "    dfCASites = dfCASites.rename(columns = {'City Name': 'City'}) \n",
    "    # Dropping Duplicates\n",
    "    dfCASites = dfCASites.drop_duplicates(subset=['City'])\n",
    "    \n",
    "    # Merging DataFrames\n",
    "    dfMerge = pd.merge(dfCASites, dfAQI, how='left', on='City')\n",
    "    # Dropping NaN\n",
    "    dfMerge = dfMerge.dropna()\n",
    "    \n",
    "    # Creating New CSV\n",
    "    dfMerge.to_csv('../Data/currentAQIData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sleeper():\n",
    "#     scrape_info()\n",
    "# while True:\n",
    "#   sleeper()\n",
    "#   tm.sleep(3600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyPythonEnv",
   "language": "python",
   "name": "mypythonenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
